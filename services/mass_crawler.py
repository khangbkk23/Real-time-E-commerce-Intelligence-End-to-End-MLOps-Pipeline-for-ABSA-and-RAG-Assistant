import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
import time
import json
import random
import pandas as pd
from datetime import datetime
from urllib.parse import quote
import sys
import os

# --- C·∫§U H√åNH ---
PROFILE_PATH = "/home/dikhang_hcmut/myshopee_profile"
MAX_PRODUCTS_PER_CAT = 10     # L·∫•y Top 10 s·∫£n ph·∫©m b√°n ch·∫°y nh·∫•t m·ªói lo·∫°i
MAX_PAGES_PER_PROD = 50       # C·ªë g·∫Øng l·∫•y t·ªõi 50 trang (kho·∫£ng 2500 review/sp)

class ShopeeMassCrawler:
    def __init__(self, headless=False):
        options = uc.ChromeOptions()
        options.add_argument(f"--user-data-dir={PROFILE_PATH}")
        options.set_capability("goog:loggingPrefs", {"performance": "ALL"})
        options.add_argument('--disable-blink-features=AutomationControlled')
        # T·∫Øt h√¨nh ·∫£nh ƒë·ªÉ load nhanh
        options.add_argument('--blink-settings=imagesEnabled=false')
        
        if headless:
            options.add_argument('--headless=new')
        
        print("üöÄ KHO·ªûI ƒê·ªòNG CRAWLER V·ªöI T√çNH NƒÇNG AUTO-SAVE...")
        self.driver = uc.Chrome(options=options, headless=headless, use_subprocess=True)
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        self.full_data = [] 
        self.current_keyword = ""

    # ---------------------------------------------------
    # H√ÄM CLICK NEXT PAGE (FIX L·ªñI KH√îNG CHUY·ªÇN TRANG)
    # ---------------------------------------------------
    def try_click_next_page(self):
        try:
            # 1. Cu·ªôn xu·ªëng ƒë√°y ƒë·ªÉ load pagination
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight - 1200);")
            time.sleep(1)

            # 2. T√¨m n√∫t Next (M≈©i t√™n ph·∫£i)
            next_buttons_xpaths = [
                "//button[contains(@class, 'shopee-icon-button--right')]", 
                "//button[@class='shopee-icon-button shopee-icon-button--right']"
            ]

            target_btn = None
            for xpath in next_buttons_xpaths:
                try:
                    btns = self.driver.find_elements(By.XPATH, xpath)
                    for btn in btns:
                        # Ki·ªÉm tra n√∫t c√≥ hi·ªÉn th·ªã v√† kh√¥ng b·ªã disabled (m·ªù ƒëi)
                        if btn.is_displayed() and btn.is_enabled():
                            target_btn = btn
                            break
                    if target_btn: break
                except: continue

            if target_btn:
                # 3. D√πng JavaScript Click (Xuy√™n v·∫≠t c·∫£n)
                self.driver.execute_script("arguments[0].click();", target_btn)
                return True
            else:
                return False

        except Exception:
            return False

    # ---------------------------------------------------
    # H√ÄM L∆ØU FILE CSV
    # ---------------------------------------------------
    def save_current_batch(self):
        if not self.full_data: 
            print("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu m·ªõi ƒë·ªÉ l∆∞u.")
            return

        try:
            print(f"\nüíæ ƒêANG L∆ØU D·ªÆ LI·ªÜU CHO: {self.current_keyword.upper()}...")
            df = pd.DataFrame(self.full_data)
            # L·ªçc tr√πng
            df = df.drop_duplicates(subset=['username', 'comment', 'timestamp'])
            # Ch·ªâ l·∫•y comment c√≥ n·ªôi dung > 5 k√Ω t·ª±
            df = df[df['comment'].str.len() > 5]
            
            # T√™n file theo t·ª´ kh√≥a + timestamp ƒë·ªÉ kh√¥ng b·ªã ghi ƒë√®
            safe_keyword = self.current_keyword.replace(' ', '_')
            filename = f"dataset_{safe_keyword}_{int(time.time())}.csv"
            
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            
            print(f"‚úÖ ƒê√É L∆ØU TH√ÄNH C√îNG: {filename}")
            print(f"üìä T·ªïng s·ªë d√≤ng: {len(df)}")
            
            # Reset buffer sau khi l∆∞u xong
            self.full_data = [] 
            
        except Exception as e:
            print(f"‚ùå L·ªói l∆∞u file: {e}")

    # ---------------------------------------------------
    # HUMAN CHECK & CAPTCHA
    # ---------------------------------------------------
    def human_like_delay(self, min_sec=2, max_sec=5):
        time.sleep(random.uniform(min_sec, max_sec))

    def check_captcha_safe(self):
        try:
            # Check nhanh
            if 'geetest' in self.driver.page_source.lower():
                self.wait_for_human()
                return

            selectors = ["//div[@class='geetest_window']", "//div[contains(text(), 'x√°c minh')]"]
            for s in selectors:
                elems = self.driver.find_elements(By.XPATH, s)
                if elems and elems[0].is_displayed():
                    self.wait_for_human()
                    return
        except: pass

    def wait_for_human(self):
        print("\n" + "!"*60)
        print("üö® PH√ÅT HI·ªÜN CAPTCHA! T·∫†M D·ª™NG.")
        print("üëâ Gi·∫£i xong nh·∫•n [ENTER] ƒë·ªÉ ch·∫°y ti·∫øp.")
        print("üëâ N·∫øu mu·ªën D·ª™NG LU√îN, nh·∫•n [Ctrl + C].")
        print("!"*60)
        sys.stdout.write('\a')
        sys.stdout.flush()
        
        # Ch·ªù user nh·∫•n Enter (ho·∫∑c Ctrl+C s·∫Ω vƒÉng ra ngo√†i)
        input("‚å®Ô∏è  ƒêang ch·ªù b·∫°n... ")
        
        print("‚úÖ Ti·∫øp t·ª•c...")
        self.human_like_delay(3, 5)

    # ---------------------------------------------------
    # CORE CRAWL LOGIC
    # ---------------------------------------------------
    def process_network_log(self, logs):
        extracted = []
        for entry in logs:
            try:
                log_msg = json.loads(entry["message"])["message"]
                if "Network.responseReceived" not in log_msg["method"]: continue
                params = log_msg["params"]
                if "get_ratings" not in params["response"]["url"]: continue
                
                res = self.driver.execute_cdp_cmd("Network.getResponseBody", {"requestId": params["requestId"]})
                body = json.loads(res['body'])
                
                if 'data' in body and 'ratings' in body['data']:
                    items = body['data']['ratings']
                    for item in items:
                        variant = item["product_items"][0].get("model_name", "") if item.get("product_items") else ""
                        extracted.append({
                            "username": item.get('author_username', ''),
                            "rating": item.get('rating_star', 5),
                            "comment": item.get('comment', ''),
                            "variant": variant,
                            "timestamp": item.get('ctime', 0),
                            "date": datetime.fromtimestamp(item.get('ctime', 0)).strftime('%Y-%m-%d'),
                            "keyword": self.current_keyword
                        })
            except: continue
        return extracted

    def crawl_single_product(self, url):
        print(f"   üì¶ SP: {url[:60]}...")
        self.driver.get(url)
        self.human_like_delay(4, 6)
        self.check_captcha_safe()

        # Click Tab ƒê√°nh Gi√°
        self.driver.execute_script("window.scrollBy(0, 500);")
        try:
            self.driver.execute_script("""
                let tabs = document.querySelectorAll("div");
                for (let t of tabs) {
                    if(t.innerText.includes("ƒê√°nh Gi√°") && t.innerText.length < 20) { t.click(); break; }
                }
            """)
            time.sleep(2)
        except: pass
        
        product_reviews = []
        page = 1
        
        # V√íNG L·∫∂P V√âT C·∫†N (WHILE TRUE)
        while True:
            # Gi·ªõi h·∫°n an to√†n
            if page > MAX_PAGES_PER_PROD:
                print(f"      üõë ƒê√£ ƒë·∫°t gi·ªõi h·∫°n {MAX_PAGES_PER_PROD} trang. D·ª´ng SP n√†y.")
                break

            # Scroll trigger API
            self.driver.execute_script("window.scrollBy(0, 1000);")
            time.sleep(1)
            self.driver.execute_script("window.scrollBy(0, 600);")
            self.human_like_delay(2, 4) 
            
            logs = self.driver.get_log("performance")
            new_data = self.process_network_log(logs)
            
            if new_data:
                product_reviews.extend(new_data)
                # In d·∫•u ch·∫•m ƒë·ªÉ bi·∫øt ƒëang ch·∫°y
                print(".", end="", flush=True)
            
            self.check_captcha_safe()

            # Th·ª≠ click Next Page
            if not self.try_click_next_page():
                print(f"\n      üõë H·∫øt trang (Page {page}).")
                break
                
            self.human_like_delay(3, 5) # Ch·ªù load trang m·ªõi
            page += 1
        
        print(f" Done ({len(product_reviews)} reviews)")
        return product_reviews

    def search_product_links(self, keyword):
        print(f"\nüîé T√¨m Top 10 B√°n Ch·∫°y: '{keyword}'...")
        url = f"https://shopee.vn/search?keyword={quote(keyword)}&sortBy=sales"
        self.driver.get(url)
        self.human_like_delay(5, 8)
        self.check_captcha_safe()
        
        for i in range(4):
            self.driver.execute_script(f"window.scrollBy(0, 1200);")
            self.human_like_delay(1, 2)
        
        links = []
        try:
            elements = self.driver.find_elements(By.CSS_SELECTOR, "a[data-sqe='link']")
            for elem in elements:
                href = elem.get_attribute("href")
                if href and "-i." in href: links.append(href)
        except: pass
        
        if not links:
            raw_links = self.driver.find_elements(By.TAG_NAME, "a")
            for l in raw_links:
                href = l.get_attribute("href")
                if href and "-i." in href and len(href) > 40: links.append(href)

        unique_links = list(set(links))[:MAX_PRODUCTS_PER_CAT]
        print(f"‚úÖ T√¨m th·∫•y {len(unique_links)} s·∫£n ph·∫©m.")
        return unique_links

    # ---------------------------------------------------
    # H√ÄM CH·∫†Y CHI·∫æN D·ªäCH (H·ªñ TR·ª¢ CTRL+C)
    # ---------------------------------------------------
    def run_multi_campaign(self, categories):
        print(f"üöÄ B·∫ÆT ƒê·∫¶U CHI·∫æN D·ªäCH: {len(categories)} DANH M·ª§C")
        print("üí° M·∫∏O: Nh·∫•n 'Ctrl + C' ƒë·ªÉ D·ª™NG v√† L∆ØU FILE ngay l·∫≠p t·ª©c.")
        
        try:
            for idx, cat in enumerate(categories):
                print(f"\n\n" + "#"*50)
                print(f"üìå DANH M·ª§C [{idx+1}/{len(categories)}]: {cat.upper()}")
                print("#"*50)
                
                self.current_keyword = cat
                links = self.search_product_links(cat)
                
                if not links: continue

                # Loop t·ª´ng s·∫£n ph·∫©m
                for p_idx, link in enumerate(links):
                    print(f"\nüî∏ [{p_idx+1}/{len(links)}] {cat}...")
                    
                    reviews = self.crawl_single_product(link)
                    self.full_data.extend(reviews)
                    
                    # Ngh·ªâ ng∆°i gi·ªØa c√°c s·∫£n ph·∫©m
                    self.human_like_delay(6, 10)

                # SAU KHI XONG 1 DANH M·ª§C -> L∆ØU FILE NGAY
                self.save_current_batch()
                
                print("üí§ Ngh·ªâ gi·∫£i lao 30s tr∆∞·ªõc khi qua danh m·ª•c m·ªõi...")
                time.sleep(30)

        except KeyboardInterrupt:
            print("\n\n" + "!"*50)
            print("üõë NG∆Ø·ªúI D√ôNG ƒê√É D·ª™NG (Ctrl + C)!")
            print("üõë ƒêang ti·∫øn h√†nh l∆∞u d·ªØ li·ªáu c√≤n trong b·ªô nh·ªõ...")
            self.save_current_batch()
            print("!"*50)

    def close(self):
        self.driver.quit()

# ---------------------------------------------------
# DANH S√ÅCH M·∫∂T H√ÄNG ƒê·ªÇ CRAWL
# ---------------------------------------------------
SHOPPING_LIST = [
    # C√¥ng ngh·ªá
    "tai nghe bluetooth", "chu·ªôt kh√¥ng d√¢y", "b√†n ph√≠m c∆°", "s·∫°c d·ª± ph√≤ng", 
    # Th·ªùi trang
    "√°o thun nam", "v√°y n·ªØ", "gi√†y sneaker", 
    # M·ªπ ph·∫©m
    "son m√¥i", "kem ch·ªëng n·∫Øng", "s·ªØa r·ª≠a m·∫∑t", 
    # Gia d·ª•ng
    "b√¨nh gi·ªØ nhi·ªát", "n·ªìi chi√™n kh√¥ng d·∫ßu", "g·∫•u b√¥ng"
]

if __name__ == "__main__":
    crawler = ShopeeMassCrawler(headless=False)
    try:
        crawler.run_multi_campaign(SHOPPING_LIST)
    except Exception as e:
        print(f"‚ùå Critical Error: {e}")
    finally:
        crawler.close()