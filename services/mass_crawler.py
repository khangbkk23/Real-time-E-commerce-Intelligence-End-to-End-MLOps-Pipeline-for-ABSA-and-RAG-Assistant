import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
import time
import json
import random
import pandas as pd
from datetime import datetime
from urllib.parse import quote
import sys
import os

# --- C·∫§U H√åNH ---
PROFILE_PATH = "./myshopee_profile_data"
MAX_PRODUCTS_PER_CAT = 10
MAX_PAGES_PER_PROD = 50

class ShopeeMassCrawler:
    def __init__(self, headless=False):
        options = uc.ChromeOptions()
        options.add_argument(f"--user-data-dir={PROFILE_PATH}")
        options.set_capability("goog:loggingPrefs", {"performance": "ALL"})
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--blink-settings=imagesEnabled=false')
        
        # Th√™m timeout ƒë·ªÉ tr√°nh l·ªói Read timed out
        options.add_argument("--dns-prefetch-disable")
        options.add_argument("--disable-gpu")

        if headless:
            options.add_argument('--headless=new')
        
        print("KH·ªûI ƒê·ªòNG CRAWLER (STABLE VERSION)...")
        self.driver = uc.Chrome(options=options, headless=headless, use_subprocess=True, version_main=144)
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        # Set timeout cho vi·ªác load trang (30 gi√¢y)
        self.driver.set_page_load_timeout(30)
        
        self.full_data = [] 
        self.current_keyword = ""
        self.seen_reviews = set()

    def try_click_next_page(self):
        print("      ‚û°Ô∏è ƒêang th·ª≠ b·∫•m Next Page...", end=" ")
        try:
            self.driver.execute_script("window.scrollBy(0, 600);")
            time.sleep(1)
            next_button_selectors = [
                "//button[contains(@class, 'shopee-icon-button--right')]",
                "//button[contains(@class, 'shopee-icon-button') and .//*[name()='svg' and contains(@class, 'icon-arrow-right')]]",
                "//div[@class='shopee-page-controller']//button[last()]"
            ]

            target_btn = None
            
            for xpath in next_button_selectors:
                try:
                    btns = self.driver.find_elements(By.XPATH, xpath)
                    for btn in btns:
                        class_attr = btn.get_attribute("class") or ""
                        disabled_attr = btn.get_attribute("disabled")
                        
                        if "disabled" in class_attr or disabled_attr is not None:
                            continue
                        
                        if btn.is_displayed():
                            target_btn = btn
                            break
                    if target_btn: break
                except: continue

            if target_btn:
                # 3. Click b·∫±ng JavaScript (M·∫°nh h∆°n click th∆∞·ªùng)
                self.driver.execute_script("arguments[0].click();", target_btn)
                print("‚úÖ Click th√†nh c√¥ng!")
                time.sleep(3) # Ch·ªù trang m·ªõi load
                return True
            else:
                print("Kh√¥ng t√¨m th·∫•y n√∫t Next (Ho·∫∑c ƒë√£ h·∫øt trang).")
                return False

        except Exception as e:
            print(f"L·ªói Next Page: {e}")
            return False

    def save_current_batch(self):
        if not self.full_data: 
            print("There isn't any new to save.")
            return

        try:
            print(f"\nSAVING DATA FOR: {self.current_keyword.upper()}...")
            df = pd.DataFrame(self.full_data)
            if 'source_url' not in df.columns:
                df['source_url'] = "Unknown"

            df = df.drop_duplicates(subset=['username', 'comment', 'timestamp'])
            df = df[df['comment'].str.len() > 10]
            
            output_folder = "./datasets/raw"
            
            if not os.path.exists(output_folder):
                os.makedirs(output_folder)
            
            safe_keyword = self.current_keyword.replace(' ', '_')
            file_name_only = f"dataset_{safe_keyword}_{int(time.time())}.csv"
            full_path = os.path.join(output_folder, file_name_only)
            
            cols = ['keyword', 'source_url', 'rating', 'date', 'variant', 'comment', 'username', 'timestamp']
            cols = [c for c in cols if c in df.columns]
            df = df[cols]
            
            df.to_csv(full_path, index=False, encoding='utf-8-sig')
            
            print(f"SAVED: {full_path}")
            print(f"Number of rows: {len(df)}")
            
            self.full_data = [] 
            
        except Exception as e:
            print(f"Error in saving file: {e}")

    def human_like_delay(self, min_sec=2, max_sec=5):
        time.sleep(random.uniform(min_sec, max_sec))

    def check_captcha_safe(self):
        try:
            if 'geetest' in self.driver.page_source.lower():
                self.wait_for_human()
                return
            selectors = ["//div[@class='geetest_window']", "//div[contains(text(), 'x√°c minh')]"]
            for s in selectors:
                if self.driver.find_elements(By.XPATH, s):
                    self.wait_for_human()
                    return
        except: pass

    def wait_for_human(self):
        print("\n" + "!"*50)
        print("üö® PH√ÅT HI·ªÜN CAPTCHA! GI·∫¢I XONG NH·∫§N ENTER.")
        print("!"*50)
        sys.stdout.write('\a')
        sys.stdout.flush()
        try: input("‚å®Ô∏è  Waiting... ")
        except KeyboardInterrupt: raise KeyboardInterrupt 
        self.human_like_delay(3, 5)

    def process_network_log(self, logs):
        extracted = []
        for entry in logs:
            try:
                log_msg = json.loads(entry["message"])["message"]
                if "Network.responseReceived" not in log_msg["method"]: continue
                params = log_msg["params"]
                if "get_ratings" not in params["response"]["url"]: continue
                
                res = self.driver.execute_cdp_cmd("Network.getResponseBody", {"requestId": params["requestId"]})
                body = json.loads(res['body'])
                
                if 'data' in body and 'ratings' in body['data']:
                    items = body['data']['ratings']
                    for item in items:
                        variant = item["product_items"][0].get("model_name", "") if item.get("product_items") else ""
                        extracted.append({
                            "username": item.get('author_username', ''),
                            "rating": item.get('rating_star', 5),
                            "comment": item.get('comment', ''),
                            "variant": variant,
                            "timestamp": item.get('ctime', 0),
                            "date": datetime.fromtimestamp(item.get('ctime', 0)).strftime('%Y-%m-%d'),
                            "keyword": self.current_keyword,
                            "source_url": self.driver.current_url
                        })
            except: continue
        return extracted

    # ---------------------------------------------------
    # H√ÄM CRAWL 1 S·∫¢N PH·∫®M (C√ì FIX L·ªñI TIMEOUT)
    # ---------------------------------------------------
    # def crawl_single_product(self, url):
    #     print(f"   üì¶ SP: {url[:60]}...")
        
    #     try:
    #         self.driver.get(url)
    #     except Exception:
    #         print("      ‚û°Ô∏è B·ªè qua (L·ªói load trang).")
    #         return

    #     self.human_like_delay(4, 6)
    #     self.check_captcha_safe()
    #     self.driver.execute_script("window.scrollBy(0, 500);")
    #     try:
    #         self.driver.execute_script("""
    #             let tabs = document.querySelectorAll("div");
    #             for (let t of tabs) {
    #                 if(t.innerText.includes("ƒê√°nh Gi√°") && t.innerText.length < 20) { t.click(); break; }
    #             }
    #         """)
    #         time.sleep(2)
    #     except: pass
        
    #     page = 1
    #     count_total = 0
    #     empty_page_count = 0
        
    #     while True:
    #         if page > MAX_PAGES_PER_PROD:
    #             print(f"D·ª´ng (Max {MAX_PAGES_PER_PROD} trang).")
    #             break

    #         # 2. Scroll trigger
    #         self.driver.execute_script("window.scrollBy(0, 1000);")
    #         time.sleep(1)
    #         self.driver.execute_script("window.scrollBy(0, 600);")
    #         self.human_like_delay(2, 4) 
            
    #         # 3. L·∫•y d·ªØ li·ªáu
    #         logs = self.driver.get_log("performance")
    #         new_data = self.process_network_log(logs)
            
    #         if new_data:
    #             self.full_data.extend(new_data)
    #             count_total += len(new_data)
    #             empty_page_count = 0
    #             print(".", end="", flush=True)
    #         else:
    #             empty_page_count += 1
    #             if empty_page_count >= 3: 
    #                 print(f"\n      üõë D·ª´ng (3 l·∫ßn kh√¥ng th·∫•y d·ªØ li·ªáu m·ªõi).")
    #                 break
            
    #         self.check_captcha_safe()
    #         if not self.try_click_next_page():
    #             print(f"\n      üõë H·∫øt trang (Page {page}).")
    #             break
                
    #         self.human_like_delay(3, 5)
    #         page += 1
        
    #     print(f" Done (+{count_total} reviews)")
    
    def crawl_single_product(self, url):
        print(f"   üì¶ SP: {url[:60]}...")
        
        try:
            self.driver.get(url)
        except Exception:
            print("      ‚û°Ô∏è B·ªè qua (L·ªói load trang).")
            return

        self.human_like_delay(4, 6)
        self.check_captcha_safe()

        # 1. Click Tab "ƒê√°nh Gi√°"
        self.driver.execute_script("window.scrollBy(0, 500);")
        try:
            self.driver.execute_script("""
                let tabs = document.querySelectorAll("div");
                for (let t of tabs) {
                    if(t.innerText.includes("ƒê√°nh Gi√°") && t.innerText.length < 30) { t.click(); break; }
                }
            """)
            time.sleep(2)
        except: pass

        # 2. ƒê·ªãnh nghƒ©a m·ª•c ti√™u
        target_filters = ["1 sao", "2 sao", "3 sao"] 
        
        # 3. Duy·ªát qua t·ª´ng b·ªô l·ªçc
        for target_name in target_filters:
            print(f"\n      üéØ Check filter: [{target_name.upper()}]...", end=" ")
            
            target_btn = None
            try:
                # T√¨m l·∫°i elements m·ªói v√≤ng l·∫∑p
                current_filters = self.driver.find_elements(By.CSS_SELECTOR, "div[class*='product-rating-overview__filter']")
                
                # Fallback n·∫øu selector tr√™n kh√¥ng th·∫•y
                if not current_filters:
                     current_filters = self.driver.find_elements(By.CSS_SELECTOR, ".product-rating-overview div")

                for btn in current_filters:
                    btn_text = btn.text.lower()
                    
                    if target_name in btn_text:
                        # [B·∫¢O V·ªÜ 1: N√â N√öT R·ªñNG]
                        # N·∫øu n√∫t ch·ª©a "(0)" ho·∫∑c k·∫øt th√∫c b·∫±ng "(0)" -> B·ªè qua
                        if "(0)" in btn_text or btn_text.strip().endswith("(0)"):
                            print(f"-> Tr·ªëng (0 review). Skip.", end="")
                            target_btn = None
                        else:
                            target_btn = btn
                        break
            except: pass

            if target_btn:
                # Click n√∫t
                self.driver.execute_script("arguments[0].click();", target_btn)
                print("‚úÖ Click!", end=" ")
                time.sleep(3)
                
                self.driver.get_log("performance") 
            else:
                print("‚ùå Next.")
                continue 

            # ---------------------------------------------------------
            # B·∫ÆT ƒê·∫¶U CRAWL DATA C·ª¶A FILTER HI·ªÜN T·∫†I
            # ---------------------------------------------------------
            page = 1
            empty_count = 0
            count_filter = 0
            
            while True:
                if page > 10: break # Gi·ªõi h·∫°n 10 trang cho 1 sao

                self.driver.execute_script("window.scrollBy(0, 1000);")
                time.sleep(1)
                self.driver.execute_script("window.scrollBy(0, 600);")
                self.human_like_delay(2, 3) 
                
                logs = self.driver.get_log("performance")
                new_data = self.process_network_log(logs)
                
                if new_data:
                    unique_batch = []
                    for item in new_data:
                        # [B·∫¢O V·ªÜ 3: L·ªåC C·ª®NG (HARD FILTER)]
                        # ƒê√¢y l√† ch·ªët ch·∫∑n cu·ªëi c√πng. N·∫øu rating > 3 -> V·ª®T NGAY.
                        current_rating = item.get('rating', 5)
                        if current_rating > 3:
                            continue

                        # Logic ch·ªëng tr√πng l·∫∑p
                        review_id = f"{item['username']}_{item['timestamp']}"
                        
                        if review_id not in self.seen_reviews:
                            self.seen_reviews.add(review_id)
                            item['source_url'] = url 
                            unique_batch.append(item)
                    
                    if unique_batch:
                        self.full_data.extend(unique_batch)
                        count_filter += len(unique_batch)
                        empty_count = 0
                        print(f"+{len(unique_batch)}", end=" ", flush=True)
                    else:
                        empty_count += 1
                else:
                    empty_count += 1

                if empty_count >= 2: break 

                self.check_captcha_safe()
                if not self.try_click_next_page(): break
                    
                page += 1
            
            print(f" -> Xong (+{count_filter} reviews)")
        
        print("Ho√†n th√†nh s·∫£n ph·∫©m.")

    # ---------------------------------------------------
    # H√ÄM T√åM KI·∫æM (PHI√äN B·∫¢N L·∫§Y T·ª™ ƒê·∫¶U - GI·ªÆ NGUY√äN TH·ª® T·ª∞)
    # ---------------------------------------------------
    def search_product_links(self, keyword):
        print(f"\nüîé T√¨m Top {MAX_PRODUCTS_PER_CAT} B√°n Ch·∫°y Nh·∫•t: '{keyword}'...")
        
        # S·∫Øp x·∫øp theo B√°n Ch·∫°y (Sales)
        url = f"https://shopee.vn/search?keyword={quote(keyword)}&sortBy=sales"
        
        try:
            self.driver.get(url)
        except Exception as e:
            print(f"‚ùå L·ªói load trang t√¨m ki·∫øm: {e}")
            return []

        self.human_like_delay(5, 8)
        self.check_captcha_safe()

        for i in range(5):
            self.driver.execute_script(f"window.scrollBy(0, 1000);")
            time.sleep(1) # Ch·ªù 1 ch√∫t cho h√¨nh ·∫£nh/link hi·ªán ra
        
        links = []
        try:
            elements = self.driver.find_elements(By.CSS_SELECTOR, "a[data-sqe='link']")
            
            for elem in elements:
                href = elem.get_attribute("href")
                if href and "-i." in href: # Link s·∫£n ph·∫©m shopee lu√¥n c√≥ chu·ªói "-i." ch·ª©a shopid v√† itemid
                    links.append(href)
        except: pass
        if not links:
            print("‚ö†Ô∏è Selector ch√≠nh kh√¥ng th·∫•y, d√πng Fallback...")
            raw_links = self.driver.find_elements(By.TAG_NAME, "a")
            for l in raw_links:
                href = l.get_attribute("href")
                if href and "-i." in href and len(href) > 40: 
                    links.append(href)
        seen = set()
        ordered_links = []
        for l in links:
            if l not in seen:
                ordered_links.append(l)
                seen.add(l)
        final_links = ordered_links[:MAX_PRODUCTS_PER_CAT]

        print(f"‚úÖ ƒê√£ ch·ªçn {len(final_links)} s·∫£n ph·∫©m (Top Sales).")
        return final_links


    def run_multi_campaign(self, categories):
        print(f"B·∫ÆT ƒê·∫¶U: {len(categories)} DANH M·ª§C")
        
        try:
            for idx, cat in enumerate(categories):
                print(f"\n\n" + "#"*50)
                print(f"DANH M·ª§C [{idx+1}/{len(categories)}]: {cat.upper()}")
                print("#"*50)
                
                self.current_keyword = cat
                links = self.search_product_links(cat)
                
                if not links: continue

                for p_idx, link in enumerate(links):
                    print(f"\nüî∏ [{p_idx+1}/{len(links)}] {cat}...")
                    
                    self.crawl_single_product(link)
                    
                    self.human_like_delay(6, 10)

                self.save_current_batch()
                print("Ngh·ªâ 30s...")
                time.sleep(30)

        except KeyboardInterrupt:
            print("\n\n" + "!"*50)
            print("NG∆Ø·ªúI D√ôNG D·ª™NG (Ctrl + C)!")
            print("ƒêang l∆∞u d·ªØ li·ªáu...")
            self.save_current_batch()
            print("!"*50)

    def close(self):
        try:
            self.driver.quit()
        except: pass

SHOPPING_LIST = [
    # C√¥ng ngh·ªá
    "robot h√∫t b·ª•i mini", 
    "ƒë·ªìng h·ªì th√¥ng minh th·ªÉ thao 99k",
    "b√†n ph√≠m c∆° gi√° r·∫ª", 
    "tai nghe ch·ªëng ·ªìn gi√° r·∫ª",
    "camera wifi gi√° r·∫ª",
    "m√†n h√¨nh gi√° r·∫ª",
]

if __name__ == "__main__":
    crawler = ShopeeMassCrawler(headless=False)
    try:
        crawler.run_multi_campaign(SHOPPING_LIST)
    except Exception as e:
        print(f"Critical System Error: {e}")
    finally:
        crawler.close()