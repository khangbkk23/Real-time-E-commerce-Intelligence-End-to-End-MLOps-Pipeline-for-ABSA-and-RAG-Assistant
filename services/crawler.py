import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import json
import random
import pandas as pd
from datetime import datetime

from captcha_service import ShopeeCaptchaSolver

# ƒê∆Ø·ªúNG D·∫™N PROFILE C·ª¶A B·∫†N (GI·ªÆ NGUY√äN)
PROFILE_PATH = "/home/dikhang_hcmut/myshopee_profile"

class ShopeeAdvancedCrawler:
    def __init__(self):
        options = uc.ChromeOptions()
        options.add_argument(f"--user-data-dir={PROFILE_PATH}")
        options.set_capability("goog:loggingPrefs", {"performance": "ALL"})
        
        print("üöÄ Kh·ªüi ƒë·ªông Crawler...")
        self.driver = uc.Chrome(options=options, headless=False, use_subprocess=True)
        self.wait = WebDriverWait(self.driver, 10)

    def process_log_entry(self, logs):
        """H√†m l·ªçc v√† tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ log network"""
        extracted_data = []
        for entry in logs:
            try:
                log = json.loads(entry["message"])["message"]
                if "Network.responseReceived" in log["method"]:
                    url = log["params"]["response"]["url"]
                    # Ch·ªâ b·∫Øt API get_ratings
                    if "get_ratings" in url:
                        req_id = log["params"]["requestId"]
                        try:
                            # L·∫•y response body
                            res = self.driver.execute_cdp_cmd("Network.getResponseBody", {"requestId": req_id})
                            body = json.loads(res['body'])
                            
                            if 'data' in body and 'ratings' in body['data']:
                                items = body['data']['ratings']
                                for item in items:
                                    # L·∫•y th√¥ng tin bi·∫øn th·ªÉ (M√†u/Size)
                                    variant = ""
                                    if item.get("product_items"):
                                        variant = item["product_items"][0].get("model_name", "")

                                    extracted_data.append({
                                        "username": item.get('author_username'),
                                        "rating": item.get('rating_star'),
                                        "comment": item.get('comment'),
                                        "variant": variant,
                                        "timestamp": item.get('ctime'),
                                        "date": datetime.fromtimestamp(item.get('ctime')).strftime('%Y-%m-%d %H:%M:%S'),
                                        "is_anonymous": item.get('anonymous'),
                                        "region": item.get('region', 'VN')
                                    })
                        except:
                            pass # B·ªè qua c√°c request l·ªói ho·∫∑c ko decode ƒë∆∞·ª£c
            except:
                pass
        return extracted_data

    def click_next_page(self):
        """T√¨m v√† click n√∫t Next trang"""
        try:
            # N√∫t Next th∆∞·ªùng l√† icon m≈©i t√™n ph·∫£i trong pagination
            next_btn = self.driver.find_element(By.XPATH, "//button[contains(@class, 'shopee-icon-button--right')]")
            if next_btn.is_enabled():
                self.driver.execute_script("arguments[0].click();", next_btn)
                return True
        except:
            return False
        return False

    def crawl(self, url, max_pages=5):
        print(f"üîó Truy c·∫≠p: {url}")
        self.driver.get(url)
        time.sleep(5) # Ch·ªù load init

        self.handle_antibot()
        all_reviews = []
        
        # 1. Click Tab ƒê√°nh gi√°
        try:
            self.driver.execute_script("""
                let tabs = document.querySelectorAll("div");
                for (let tab of tabs) {
                    if (tab.innerText.includes("ƒê√°nh Gi√°") && tab.innerText.length < 20) {
                        tab.click(); break;
                    }
                }
            """)
            time.sleep(3)
        except: pass

        # 2. V√≤ng l·∫∑p ph√¢n trang
        for page in range(1, max_pages + 1):
            print(f"üìÑ ƒêang x·ª≠ l√Ω trang {page}...")
            
            # Scroll nh·∫π ƒë·ªÉ trigger load (quan tr·ªçng)
            self.driver.execute_script("window.scrollBy(0, 600);")
            time.sleep(2)
            self.driver.execute_script("window.scrollBy(0, 400);")
            time.sleep(3) # Ch·ªù API ph·∫£n h·ªìi
            
            # L·∫•y Logs & Parse
            logs = self.driver.get_log("performance")
            new_data = self.process_log_entry(logs)
            
            if new_data:
                print(f"   -> B·∫Øt ƒë∆∞·ª£c {len(new_data)} reviews t·ª´ network.")
                all_reviews.extend(new_data)
            else:
                print("   -> Kh√¥ng th·∫•y g√≥i tin API n√†o.")

            # Th·ª≠ sang trang ti·∫øp theo
            if page < max_pages:
                if self.click_next_page():
                    print("   -> ƒê√£ click Next page. Ch·ªù load...")
                    time.sleep(random.uniform(3, 5))
                else:
                    print("üõë Kh√¥ng t√¨m th·∫•y n√∫t Next ho·∫∑c ƒë√£ h·∫øt trang.")
                    break
        
        return all_reviews

    def save_csv(self, data, filename="shopee_full_reviews.csv"):
        if not data:
            print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u.")
            return
            
        # Deduplicate: Lo·∫°i b·ªè c√°c d√≤ng tr√πng l·∫∑p d·ª±a tr√™n username v√† comment
        df = pd.DataFrame(data)
        initial_len = len(df)
        df = df.drop_duplicates(subset=['username', 'comment', 'timestamp'])
        
        print(f"üìä T·ªïng thu ƒë∆∞·ª£c: {initial_len} | Sau khi l·ªçc tr√πng: {len(df)}")
        
        # S·∫Øp x·∫øp c·ªôt cho ƒë·∫πp
        cols = ['date', 'username', 'rating', 'variant', 'comment', 'timestamp']
        df = df[cols]
        
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"üíæ ƒê√£ l∆∞u file: {filename}")

    def close(self):
        self.driver.quit()
    
    def handle_antibot(self):
        """H√†m check v√† x·ª≠ l√Ω bot"""
        solver = ShopeeCaptchaSolver(self.driver)
        
        # N·∫øu th·∫•y captcha
        if solver.is_captcha_present():
            print("‚ö†Ô∏è B·ªã ch·∫∑n b·ªüi Slider Captcha.")
            
            # Th·ª≠ gi·∫£i t·ªëi ƒëa 3 l·∫ßn
            for i in range(3):
                print(f"üîÑ Th·ª≠ gi·∫£i l·∫ßn {i+1}...")
                if solver.solve():
                    print("üéâ Gi·∫£i th√†nh c√¥ng, ti·∫øp t·ª•c crawl.")
                    time.sleep(3)
                    return True
                else:
                    # N·∫øu th·∫•t b·∫°i, refresh trang ƒë·ªÉ l·∫•y h√¨nh m·ªõi d·ªÖ h∆°n
                    self.driver.refresh()
                    time.sleep(5)
            
            return False
        return True

# --- MAIN RUN ---
if __name__ == "__main__":
    crawler = ShopeeAdvancedCrawler()
    try:
        # Thay link s·∫£n ph·∫©m c·ªßa b·∫°n v√†o ƒë√¢y
        product_url = "https://shopee.vn/%C3%81o-Kho%C3%A1c-D%C3%B9-Ch%E1%BB%91ng-N%E1%BA%AFng-Nam-Couple-TX-UV-Pro-Windbreaker-MOK-1058-i.83192592.28400443877"
        
        # Ch·∫°y crawl 5 trang
        data = crawler.crawl(product_url, max_pages=5)
        crawler.save_csv(data)
        
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
    finally:
        crawler.close()